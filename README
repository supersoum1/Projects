READ ME

1. Gitlet 



2. 2048

3. BYOW

My task was to design and implement a 2D tile-based world exploration engine. By "tile-based," it meant that the worlds I crafted consisted 
of a 2D grid of tiles, establishing a methodical yet adaptable framework for world design. The "world exploration engine" aspect of this
endeavour required me to develop a world that users could navigate, allowing for exploration and interaction with various objects within this
world, all from an overhead perspective.

This project was an exhilarating challenge that married game design elements with the nuances of software engineering. I was tasked with
contemplating both the technical execution of the tile-based system and the user experience of exploration and interaction. Although the 
complexity of my project did not rival that of highly sophisticated systems like the NES game "Zelda II"—which served as an inspirational
example—it was an engaging opportunity to dive into the mechanics of world-building and interactive design. My objective was to create a
compelling and immersive environment that captured the spirit of exploration and adventure, albeit on a more modest scale.

4. Deque

5. Predicting_housing_prices

In project A1, I engaged in basic Exploratory Data Analysis (EDA), which set the foundation for my approach to modeling decisions. I explored
the dataset, identified patterns, and made critical observations that informed my subsequent actions. During this process, I also expanded 
the dataset with new features and undertook a thorough data cleaning exercise to enhance the dataset's quality and relevance.

Building on that groundwork, my next task was to focus on specifying and fitting a linear model using select features of the housing data to
predict house prices. This step involved a detailed analysis of the data to determine which features were most likely to influence house 
prices, followed by the application of linear regression techniques to model these relationships. After developing the model, I meticulously
examined its error metrics to understand its performance better and identify areas for improvement. This analysis was crucial for iterating 
on the model to enhance its accuracy and reliability.

The project then took a deeper dive into the broader implications of predictive modeling, particularly within the context of the Cook County
Assessor's Office (CCAO) case study. Given that the CCAO relies on statistical modeling for property valuation, it was imperative to consider
the historical context of racial discrimination in housing policy and property taxation within Cook County. This reflection was critical for
assessing the impact of my modeling efforts and pondering the notion of fairness for property owners in the county. Such considerations were
vital for ensuring that the modeling practices adopted could contribute positively to addressing longstanding inequities.

Throughout this project, I became proficient in:
  -Implementing a data processing pipeline using pandas, which included data cleaning, feature engineering, and preparation for modeling.
  -Utilizing scikit-learn to construct and fit linear models, gaining valuable experience in applying this popular machine learning library to solve real-world problems.
  -This experience was not just about developing technical skills but also about understanding the ethical considerations and potential societal impacts of predictive modeling, especially in sensitive areas like property valuation.


6. SpamHam


The project required setting up a Python environment and installing all dependencies, laying the groundwork for the SVM 
classification tasks ahead. I worked with three datasets: a synthetic toy dataset, the widely recognized MNIST dataset of handwritten digits,
and a spam dataset featuring featurized email data. Each dataset presented its unique challenges, from data loading and preprocessing to
model training and evaluation.

Training linear SVMs on the MNIST and spam datasets was a highlight of this project, allowing me to explore the effects of training size on
model accuracy. This exercise was about applying SVMs and understanding the nuances of machine learning models, including
the balance between complexity and performance.

Data partitioning and evaluation metrics were crucial components of my project. I developed strategies for managing datasets and adopted 
classification accuracy as a primary measure of model performance. Hyperparameter tuning was another area of focus, where I experimented 
with different values of the regularization parameter C to optimize the soft-margin SVM algorithm. This enlightening process revealed
the delicate interplay between model parameters and performance.

I also ventured into k-fold cross-validation, particularly for the smaller spam dataset, to mitigate the high variance in accuracy
estimates. This method proved invaluable, offering insights into the robustness of machine learning models across different data partitions.

The culmination of my project was participating in Kaggle competitions for both the MNIST and spam datasets. This experience was not just
about testing my models against unseen data but also about engaging in a broader community of machine learning practitioners. I explored 
feature engineering and other techniques to enhance my models, always within the confines of using SVMs as mandated by the project guidelines.

....
